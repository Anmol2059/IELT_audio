{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9826f30327754be3b17b57dcbaf44d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1b8bab348b1416bbc25f2968e3f7ae0",
              "IPY_MODEL_dfc07ccc89404cd0ac9794de8efb60e8",
              "IPY_MODEL_5d199c96282c45c2a35ad5e5fa9f3ec5"
            ],
            "layout": "IPY_MODEL_bfefd3fd073f43608d8efdf7b79851ae"
          }
        },
        "d1b8bab348b1416bbc25f2968e3f7ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f63ad22fc8c49daba3eacd9a1d59c3a",
            "placeholder": "​",
            "style": "IPY_MODEL_9ad493a0046947bc96a4d7e76e17d0b6",
            "value": "config.json: 100%"
          }
        },
        "dfc07ccc89404cd0ac9794de8efb60e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f245c685656f4f10bf601a4b29819e07",
            "max": 1596,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0e4f82c570a4f76b23f12589eb17713",
            "value": 1596
          }
        },
        "5d199c96282c45c2a35ad5e5fa9f3ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fd27728860a4ab2824d1542b6cde0d4",
            "placeholder": "​",
            "style": "IPY_MODEL_bc206a769d8646d9937a068b7d3d210b",
            "value": " 1.60k/1.60k [00:00&lt;00:00, 23.5kB/s]"
          }
        },
        "bfefd3fd073f43608d8efdf7b79851ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f63ad22fc8c49daba3eacd9a1d59c3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad493a0046947bc96a4d7e76e17d0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f245c685656f4f10bf601a4b29819e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0e4f82c570a4f76b23f12589eb17713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fd27728860a4ab2824d1542b6cde0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc206a769d8646d9937a068b7d3d210b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64f5b6922406427893e2f02a12197693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb30ac024c21413d9d8ed02939ffdf09",
              "IPY_MODEL_8cea647026b7412f9b81cc4eec0c7b7b",
              "IPY_MODEL_18bc61d866264de18e441e5137b9acbe"
            ],
            "layout": "IPY_MODEL_d60c103c7f934f8e8205b6c384f3e546"
          }
        },
        "cb30ac024c21413d9d8ed02939ffdf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce9c9a9b62e4cf7919827a6594361b6",
            "placeholder": "​",
            "style": "IPY_MODEL_0c365dcc9b484f1c8d40ba60cc5219e7",
            "value": "model.safetensors: 100%"
          }
        },
        "8cea647026b7412f9b81cc4eec0c7b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_045d088674fd4592a573754c31571df5",
            "max": 377607901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_673ec031b088447e913548951252c478",
            "value": 377607901
          }
        },
        "18bc61d866264de18e441e5137b9acbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b442cee8c9744299665e94d573ae265",
            "placeholder": "​",
            "style": "IPY_MODEL_147bc10926924cabb0005e7ce3cf6f66",
            "value": " 378M/378M [00:02&lt;00:00, 284MB/s]"
          }
        },
        "d60c103c7f934f8e8205b6c384f3e546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dce9c9a9b62e4cf7919827a6594361b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c365dcc9b484f1c8d40ba60cc5219e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "045d088674fd4592a573754c31571df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "673ec031b088447e913548951252c478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b442cee8c9744299665e94d573ae265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "147bc10926924cabb0005e7ce3cf6f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf\n",
        "!pip install conformer\n",
        "!pip install ml-collections\n",
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQPF9OHta9oh",
        "outputId": "2b5d4bb9-b034-4456-da63-801b48735884"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: conformer in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: einops>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from conformer) (0.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from conformer) (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->conformer) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->conformer) (1.3.0)\n",
            "Collecting ml-collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections) (6.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections) (21.6.0)\n",
            "Building wheels for collected packages: ml-collections\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94507 sha256=04a3f362e143cc08e6e97f92f33655aa82ac269c8e5616ea9559f99f371cc73f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built ml-collections\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.1\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required Classes and definition from IELT"
      ],
      "metadata": {
        "id": "_GuljHulslte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iERd295WaRPV"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "\n",
        "import copy\n",
        "import math\n",
        "from os.path import join as pjoin\n",
        "import ml_collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "\t\"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "\tif conv:\n",
        "\t\tweights = weights.transpose([3, 2, 0, 1])\n",
        "\treturn torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "\treturn x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "\tdef __init__(self, config):\n",
        "\t\tsuper(Mlp, self).__init__()\n",
        "\t\tself.fc1 = Linear(config.hidden_size, config.mlp_dim)\n",
        "\t\tself.fc2 = Linear(config.mlp_dim, config.hidden_size)\n",
        "\t\tself.act_fn = ACT2FN[\"gelu\"]\n",
        "\t\tself.dropout = Dropout(config.dropout_rate)\n",
        "\n",
        "\t\tself._init_weights()\n",
        "\n",
        "\tdef _init_weights(self):\n",
        "\t\tnn.init.xavier_uniform_(self.fc1.weight)\n",
        "\t\tnn.init.xavier_uniform_(self.fc2.weight)\n",
        "\t\tnn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "\t\tnn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = self.act_fn(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\tx = self.fc2(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "\t\"\"\"Construct the embeddings from patch, position embeddings.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, config, img_size, in_channels=3):\n",
        "\t\tsuper(Embeddings, self).__init__()\n",
        "\t\timg_size = _pair(img_size)\n",
        "\n",
        "\t\tpatch_size = _pair(config.patches)\n",
        "\t\tn_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "\t\tself.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "\t\t                               out_channels=config.hidden_size,\n",
        "\t\t                               kernel_size=patch_size,\n",
        "\t\t                               stride=patch_size)\n",
        "\t\tself.position_embeddings = nn.Parameter(torch.zeros(1, n_patches + 1, config.hidden_size))\n",
        "\t\tself.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "\t\tself.dropout = Dropout(config.dropout_rate)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tB = x.shape[0]\n",
        "\t\tcls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "\t\tx = self.patch_embeddings(x)\n",
        "\t\tx = x.flatten(2)\n",
        "\t\tx = x.transpose(-1, -2)\n",
        "\t\tx = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "\t\tembeddings = x + self.position_embeddings\n",
        "\t\tembeddings = self.dropout(embeddings)\n",
        "\t\treturn embeddings\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\tdef __init__(self, config):\n",
        "\t\tsuper(Encoder, self).__init__()\n",
        "\t\tself.layer = nn.ModuleList()\n",
        "\t\t# for _ in range(config.num_layers):\n",
        "\t\tfor _ in range(config.num_layers + 1):\n",
        "\t\t\tlayer = Block(config)\n",
        "\t\t\tself.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "\tdef forward(self, hidden_states):\n",
        "\t\t# attmap = []\n",
        "\t\tfor layer in self.layer:\n",
        "\t\t\thidden_states, weights = layer(hidden_states)\n",
        "\t\t# print(weights.shape)\n",
        "\t\t# attmap.append(weights)\n",
        "\t\treturn hidden_states\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\tdef __init__(self, config, img_size):\n",
        "\t\tsuper(Transformer, self).__init__()\n",
        "\t\tself.embeddings = Embeddings(config, img_size=img_size)\n",
        "\t\tself.encoder = Encoder(config)\n",
        "\n",
        "\tdef forward(self, input_ids):\n",
        "\t\tembedding_output = self.embeddings(input_ids)\n",
        "\t\tpart_encoded = self.encoder(embedding_output)\n",
        "\t\treturn part_encoded\n",
        "\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "\t\"\"\"\n",
        "\tNLL loss with label smoothing.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, smoothing=0.0):\n",
        "\t\t\"\"\"\n",
        "\t\tConstructor for the LabelSmoothing module.\n",
        "\t\tparam smoothing: label smoothing factor\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(LabelSmoothing, self).__init__()\n",
        "\t\tself.confidence = 1.0 - smoothing\n",
        "\t\tself.smoothing = smoothing\n",
        "\n",
        "\tdef forward(self, x, target):\n",
        "\t\tlogprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
        "\t\tnll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "\t\tnll_loss = nll_loss.squeeze(1)\n",
        "\t\tsmooth_loss = -logprobs.mean(dim=-1)\n",
        "\t\tloss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "\t\treturn loss.mean()\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\tdef __init__(self, config, assess=False):\n",
        "\t\tsuper(Attention, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.num_attention_heads = config.num_heads\n",
        "\t\tself.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "\t\tself.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "\t\tself.query = Linear(config.hidden_size, self.all_head_size)\n",
        "\t\tself.key = Linear(config.hidden_size, self.all_head_size)\n",
        "\t\tself.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "\t\tself.out = Linear(config.hidden_size, config.hidden_size)\n",
        "\t\tself.attn_dropout = Dropout(config.att_dropout)\n",
        "\t\tself.proj_dropout = Dropout(config.att_dropout)\n",
        "\n",
        "\t\tself.softmax = Softmax(dim=-1)\n",
        "\n",
        "\tdef transpose_for_scores(self, x):\n",
        "\t\tnew_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "\t\tx = x.view(*new_x_shape)\n",
        "\t\treturn x.permute(0, 2, 1, 3)\n",
        "\n",
        "\tdef forward(self, hidden_states):\n",
        "\t\tmixed_query_layer = self.query(hidden_states)\n",
        "\t\tmixed_key_layer = self.key(hidden_states)\n",
        "\t\tmixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "\t\tquery_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\t\tkey_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "\t\tvalue_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "\t\tattention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\t\tattention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\t\tattention_probs = self.softmax(attention_scores)\n",
        "\t\tweights = attention_probs\n",
        "\t\tattention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "\t\tcontext_layer = torch.matmul(attention_probs, value_layer)\n",
        "\t\tcontext_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "\t\tnew_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "\t\tcontext_layer = context_layer.view(*new_context_layer_shape)\n",
        "\t\tattention_output = self.out(context_layer)\n",
        "\t\tattention_output = self.proj_dropout(attention_output)\n",
        "\t\tif self.assess:\n",
        "\t\t\treturn attention_output, weights, attention_scores\n",
        "\t\telse:\n",
        "\t\t\treturn attention_output, weights\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\tdef __init__(self, config, assess=False):\n",
        "\t\tsuper(Block, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.hidden_size = config.hidden_size\n",
        "\t\tself.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\t\tself.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\t\tself.ffn = Mlp(config)\n",
        "\t\tself.attn = Attention(config, self.assess)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\th = x\n",
        "\t\tx = self.attention_norm(x)\n",
        "\t\tif self.assess:\n",
        "\t\t\tx, weights, score = self.attn(x)\n",
        "\t\telse:\n",
        "\t\t\tx, weights = self.attn(x)\n",
        "\t\tx = x + h\n",
        "\n",
        "\t\th = x\n",
        "\t\tx = self.ffn_norm(x)\n",
        "\t\tx = self.ffn(x)\n",
        "\t\tx = x + h\n",
        "\t\treturn x, weights\n",
        "\n",
        "\tdef load_from(self, weights, n_block):\n",
        "\t\tROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tquery_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size,\n",
        "\t\t\t                                                                       self.hidden_size).t()\n",
        "\t\t\tkey_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\t\t\tvalue_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size,\n",
        "\t\t\t                                                                       self.hidden_size).t()\n",
        "\t\t\tout_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size,\n",
        "\t\t\t                                                                       self.hidden_size).t()\n",
        "\n",
        "\t\t\tquery_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "\t\t\tkey_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "\t\t\tvalue_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "\t\t\tout_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "\t\t\tself.attn.query.weight.copy_(query_weight)\n",
        "\t\t\tself.attn.key.weight.copy_(key_weight)\n",
        "\t\t\tself.attn.value.weight.copy_(value_weight)\n",
        "\t\t\tself.attn.out.weight.copy_(out_weight)\n",
        "\t\t\tself.attn.query.bias.copy_(query_bias)\n",
        "\t\t\tself.attn.key.bias.copy_(key_bias)\n",
        "\t\t\tself.attn.value.bias.copy_(value_bias)\n",
        "\t\t\tself.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "\t\t\tmlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "\t\t\tmlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "\t\t\tmlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "\t\t\tmlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "\t\t\tself.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "\t\t\tself.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "\t\t\tself.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "\t\t\tself.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "\t\t\tself.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "\t\t\tself.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "\t\t\tself.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "\t\t\tself.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "# from core.vit import *\n",
        "# config = get_b16_config()\n",
        "# import ml_collections\n",
        "def get_b16_config():\n",
        "\t\"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "\tconfig = ml_collections.ConfigDict()\n",
        "\tconfig.patches = (16, 16)\n",
        "\tconfig.hidden_size = 768\n",
        "\tconfig.mlp_dim = 3072\n",
        "\tconfig.num_heads = 12\n",
        "\tconfig.num_layers = 12\n",
        "\tconfig.att_dropout = 0.0\n",
        "\tconfig.dropout_rate = 0.1\n",
        "\tconfig.classifier = 'token'\n",
        "\treturn config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for Image exact Implementation"
      ],
      "metadata": {
        "id": "bBHXuaaYlfw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "# from models.modules import *\n",
        "# from models.vit import get_b16_config\n",
        "\n",
        "\n",
        "class InterEnsembleLearningTransformer(nn.Module):\n",
        "\tdef __init__(self, config, img_size=448, num_classes=2, dataset='cub', smooth_value=0.,\n",
        "\t             loss_alpha=0.4, cam=True, dsm=True, fix=True, update_warm=500,\n",
        "\t             vote_perhead=24, total_num=126, assess=False):\n",
        "\t\tsuper(InterEnsembleLearningTransformer, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.smooth_value = smooth_value\n",
        "\t\tself.num_classes = num_classes\n",
        "\t\tself.loss_alpha = loss_alpha\n",
        "\t\tself.cam = cam\n",
        "\n",
        "\t\tself.embeddings = Embeddings(config, img_size=img_size)\n",
        "\t\tself.encoder = IELTEncoder(config, update_warm, vote_perhead, dataset, cam, dsm,\n",
        "\t\t                           fix, total_num, assess)\n",
        "\t\tself.head = Linear(config.hidden_size, num_classes)\n",
        "\t\tself.softmax = Softmax(dim=-1)\n",
        "\n",
        "\n",
        "\tdef forward(self, x, labels=None):\n",
        "\t\ttest_mode = False if labels is not None else True\n",
        "\t\tx = self.embeddings(x)\n",
        "\t\tif self.assess:\n",
        "\t\t\tx, xc, assess_list = self.encoder(x, test_mode)\n",
        "\t\telse:\n",
        "\t\t\tx, xc = self.encoder(x, test_mode)\n",
        "\n",
        "\t\tif self.cam:\n",
        "\t\t\tcomplement_logits = self.head(xc)\n",
        "\t\t\tprobability = self.softmax(complement_logits)\n",
        "\t\t\tweight = self.head.weight\n",
        "\t\t\tassist_logit = probability * (weight.sum(-1))\n",
        "\t\t\tpart_logits = self.head(x) + assist_logit\n",
        "\t\telse:\n",
        "\t\t\tpart_logits = self.head(x)\n",
        "\n",
        "\t\tif self.assess:\n",
        "\t\t\treturn part_logits, assess_list\n",
        "\n",
        "\t\telif test_mode:\n",
        "\t\t\treturn part_logits\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tif self.smooth_value == 0:\n",
        "\t\t\t\tloss_fct = CrossEntropyLoss()\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss_fct = LabelSmoothing(self.smooth_value)\n",
        "\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tloss_p = loss_fct(part_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\t\tloss_c = loss_fct(complement_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\t\talpha = self.loss_alpha\n",
        "\t\t\t\tloss = (1 - alpha) * loss_p + alpha * loss_c\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss = loss_fct(part_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\treturn part_logits, loss\n",
        "\n",
        "\tdef get_eval_data(self):\n",
        "\t\treturn self.encoder.select_num\n",
        "\n",
        "\tdef load_from(self, weights):\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tnn.init.zeros_(self.head.weight)\n",
        "\t\t\tnn.init.zeros_(self.head.bias)\n",
        "\n",
        "\t\t\tself.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "\t\t\tself.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "\t\t\tself.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "\t\t\t# self.encoder.patch_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "\t\t\t# self.encoder.patch_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\t\t\t# self.encoder.clr_encoder.patch_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "\t\t\t# self.encoder.clr_encoder.patch_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "\t\t\tposemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "\t\t\tposemb_new = self.embeddings.position_embeddings\n",
        "\t\t\tif posemb.size() == posemb_new.size():\n",
        "\t\t\t\tself.embeddings.position_embeddings.copy_(posemb)\n",
        "\t\t\telse:\n",
        "\t\t\t\tntok_new = posemb_new.size(1)\n",
        "\n",
        "\t\t\t\tposemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "\t\t\t\tntok_new -= 1\n",
        "\n",
        "\t\t\t\tgs_old = int(np.sqrt(len(posemb_grid)))\n",
        "\t\t\t\tgs_new = int(np.sqrt(ntok_new))\n",
        "\t\t\t\t# print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "\t\t\t\tposemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "\t\t\t\tzoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "\t\t\t\tposemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "\t\t\t\tposemb_grid = posemb_grid.reshape((1, gs_new * gs_new, -1))\n",
        "\t\t\t\tposemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "\t\t\t\tself.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "\t\t\tfor bname, block in self.encoder.named_children():\n",
        "\t\t\t\tfor uname, unit in block.named_children():\n",
        "\t\t\t\t\tif not bname.startswith('key') and not bname.startswith('clr'):\n",
        "\t\t\t\t\t\tif uname == '12':\n",
        "\t\t\t\t\t\t\tuname = '11'\n",
        "\t\t\t\t\t\tunit.load_from(weights, n_block=uname)\n",
        "\n",
        "\n",
        "class MultiHeadVoting(nn.Module):\n",
        "\tdef __init__(self, config, vote_perhead=24, fix=True):\n",
        "\t\tsuper(MultiHeadVoting, self).__init__()\n",
        "\t\tself.fix = fix\n",
        "\t\tself.num_heads = config.num_heads\n",
        "\t\tself.vote_perhead = vote_perhead\n",
        "\n",
        "\t\tif self.fix:\n",
        "\t\t\tself.kernel = torch.tensor([[1, 2, 1],\n",
        "\t\t\t                            [2, 4, 2],\n",
        "\t\t\t                            [1, 2, 1]], device='cuda').unsqueeze(0).unsqueeze(0).half()\n",
        "\t\t\tself.conv = F.conv2d\n",
        "\t\telse:\n",
        "\t\t\tself.conv = nn.Conv2d(1, 1, 3, 1, 1)\n",
        "\n",
        "\tdef forward(self, x, select_num=None, last=False):\n",
        "\t\tB, patch_num = x.shape[0], x.shape[3] - 1\n",
        "\t\tselect_num = self.vote_perhead if select_num is None else select_num\n",
        "\t\tcount = torch.zeros((B, patch_num), dtype=torch.int, device='cuda').half()\n",
        "\t\tscore = x[:, :, 0, 1:]\n",
        "\t\t_, select = torch.topk(score, self.vote_perhead, dim=-1)\n",
        "\t\tselect = select.reshape(B, -1)\n",
        "\n",
        "\t\tfor i, b in enumerate(select):\n",
        "\t\t\tcount[i, :] += torch.bincount(b, minlength=patch_num)\n",
        "\n",
        "\t\tif not last:\n",
        "\t\t\tcount = self.enhace_local(count)\n",
        "\t\t\tpass\n",
        "\n",
        "\t\tpatch_value, patch_idx = torch.sort(count, dim=-1, descending=True)\n",
        "\t\tpatch_idx += 1\n",
        "\t\treturn patch_idx[:, :select_num], count\n",
        "\n",
        "\tdef enhace_local(self, count):\n",
        "\t\tB, H = count.shape[0], math.ceil(math.sqrt(count.shape[1]))\n",
        "\t\tcount = count.reshape(B, H, H)\n",
        "\t\tif self.fix:\n",
        "\t\t\tcount = self.conv(count.unsqueeze(1), self.kernel, stride=1, padding=1).reshape(B, -1)\n",
        "\t\telse:\n",
        "\t\t\tcount = self.conv(count.unsqueeze(1)).reshape(B, -1)\n",
        "\t\treturn count\n",
        "\n",
        "\n",
        "class CrossLayerRefinement(nn.Module):\n",
        "\tdef __init__(self, config, clr_layer):\n",
        "\t\tsuper(CrossLayerRefinement, self).__init__()\n",
        "\t\tself.clr_layer = clr_layer\n",
        "\t\tself.clr_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "\tdef forward(self, x, cls):\n",
        "\t\tout = [torch.stack(token) for token in x]\n",
        "\t\tout = torch.stack(out).squeeze(1)\n",
        "\t\tout = torch.cat((cls, out), dim=1)\n",
        "\t\tout, weights = self.clr_layer(out)\n",
        "\t\tout = self.clr_norm(out)\n",
        "\t\treturn out, weights\n",
        "\n",
        "\n",
        "class IELTEncoder(nn.Module):\n",
        "\tdef __init__(self, config, update_warm=500, vote_perhead=24, dataset='cub',\n",
        "\t             cam=True, dsm=True, fix=True, total_num=126, assess=False):\n",
        "\t\tsuper(IELTEncoder, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.warm_steps = update_warm\n",
        "\t\tself.layer = nn.ModuleList()\n",
        "\t\tself.layer_num = config.num_layers\n",
        "\t\tself.vote_perhead = vote_perhead\n",
        "\t\tself.dataset = dataset\n",
        "\t\tself.cam = cam\n",
        "\t\tself.dsm = dsm\n",
        "\n",
        "\t\tfor _ in range(self.layer_num - 1):\n",
        "\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\n",
        "\t\tif self.dataset == 'dog' or self.dataset == 'nabrids':\n",
        "\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\t\t\tself.clr_layer = self.layer[-1]\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\t\t\t\tself.key_layer = self.layer[-1]\n",
        "\t\telse:\n",
        "\t\t\tself.clr_layer = Block(config)\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tself.key_layer = Block(config)\n",
        "\n",
        "\t\tif self.cam:\n",
        "\t\t\tself.key_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "\t\tself.patch_select = MultiHeadVoting(config, self.vote_perhead, fix)\n",
        "\n",
        "\t\tself.total_num = total_num\n",
        "\t\t## for CUB and NABirds\n",
        "\t\tself.select_rate = torch.tensor([16, 14, 12, 10, 8, 6, 8, 10, 12, 14, 16], device='cuda') / self.total_num\n",
        "\t\t## for Others\n",
        "\t\t# self.select_rate = torch.ones(self.layer_num-1,device='cuda')/(self.layer_num-1)\n",
        "\n",
        "\t\tself.select_num = self.select_rate * self.total_num\n",
        "\t\tself.clr_encoder = CrossLayerRefinement(config, self.clr_layer)\n",
        "\t\tself.count = 0\n",
        "\n",
        "\tdef forward(self, hidden_states, test_mode=False):\n",
        "\t\tif not test_mode:\n",
        "\t\t\tself.count += 1\n",
        "\t\tB, N, C = hidden_states.shape\n",
        "\t\tcomplements = [[] for i in range(B)]\n",
        "\t\tclass_token_list = []\n",
        "\t\tif self.assess:\n",
        "\t\t\tlayer_weights = []\n",
        "\t\t\tlayer_selected = []\n",
        "\t\t\tlayer_score = []\n",
        "\t\telse:\n",
        "\t\t\tpass\n",
        "\n",
        "\t\tfor t in range(self.layer_num - 1):\n",
        "\t\t\tlayer = self.layer[t]\n",
        "\t\t\tselect_num = torch.round(self.select_num[t]).int()\n",
        "\t\t\thidden_states, weights = layer(hidden_states)\n",
        "\t\t\tselect_idx, select_score = self.patch_select(weights, select_num)\n",
        "\t\t\tfor i in range(B):\n",
        "\t\t\t\tcomplements[i].extend(hidden_states[i, select_idx[i, :]])\n",
        "\t\t\tclass_token_list.append(hidden_states[:, 0].unsqueeze(1))\n",
        "\t\t\tif self.assess:\n",
        "\t\t\t\tlayer_weights.append(weights)\n",
        "\t\t\t\tlayer_score.append(select_score)\n",
        "\t\t\t\tlayer_selected.extend(select_idx)\n",
        "\t\tcls_token = hidden_states[:, 0].unsqueeze(1)\n",
        "\n",
        "\t\tclr, weights = self.clr_encoder(complements, cls_token)\n",
        "\t\tsort_idx, _ = self.patch_select(weights, select_num=24, last=True)\n",
        "\n",
        "\t\tif not test_mode and self.count >= self.warm_steps and self.dsm:\n",
        "\t\t\t# if not test_mode and self.count >= 500 and self.dsm:\n",
        "\t\t\tlayer_count = self.count_patch(sort_idx)\n",
        "\t\t\tself.update_layer_select(layer_count)\n",
        "\n",
        "\t\tclass_token_list = torch.cat(class_token_list, dim=1)\n",
        "\n",
        "\t\tif not self.cam:\n",
        "\t\t\treturn clr[:, 0], None\n",
        "\t\telse:\n",
        "\t\t\tout = []\n",
        "\t\t\tfor i in range(B):\n",
        "\t\t\t\tout.append(clr[i, sort_idx[i, :]])\n",
        "\t\t\tout = torch.stack(out).squeeze(1)\n",
        "\t\t\tout = torch.cat((cls_token, out), dim=1)\n",
        "\t\t\tout, _ = self.key_layer(out)\n",
        "\t\t\tkey = self.key_norm(out)\n",
        "\n",
        "\t\tif self.assess:\n",
        "\t\t\tassess_list = [layer_weights, layer_selected, layer_score, sort_idx]\n",
        "\t\t\treturn key[:, 0], clr[:, 0], assess_list\n",
        "\t\telse:\n",
        "\n",
        "\t\t\t# fused = torch.cat((class_token_list, clr[:, 0].unsqueeze(1)), dim=1)\n",
        "\t\t\t# clr[:, 0] = fused.mean(1)\n",
        "\t\t\treturn key[:, 0], clr[:, 0]\n",
        "\n",
        "\tdef update_layer_select(self, layer_count):\n",
        "\t\talpha = 1e-3  # if self.dataset != 'dog' and self.dataset == 'nabirds' else 1e-4\n",
        "\t\tnew_rate = layer_count / layer_count.sum()\n",
        "\n",
        "\t\tself.select_rate = self.select_rate * (1 - alpha) + alpha * new_rate\n",
        "\t\tself.select_rate /= self.select_rate.sum()\n",
        "\t\tself.select_num = self.select_rate * self.total_num\n",
        "\n",
        "\tdef count_patch(self, sort_idx):\n",
        "\t\tlayer_count = torch.cumsum(self.select_num, dim=-1)\n",
        "\t\tsort_idx = (sort_idx - 1).reshape(-1)\n",
        "\t\tfor i in range(self.layer_num - 1):\n",
        "\t\t\tmask = (sort_idx < layer_count[i])\n",
        "\t\t\tlayer_count[i] = mask.sum()\n",
        "\t\tcum_count = torch.cat((torch.tensor([0], device='cuda'), layer_count[:-1]))\n",
        "\t\tlayer_count -= cum_count\n",
        "\t\treturn layer_count.int()\n",
        "\n",
        "\t## Old Implementation\n",
        "\t# layer_count = torch.zeros(self.layer_num, device='cuda').int()\n",
        "\t# sort_idx = (sort_idx - 1).reshape(-1)\n",
        "\t# sorted, _ = torch.sort(sort_idx)\n",
        "\t# for j in range(self.layer_num):\n",
        "\t# \tif j == (self.layer_num - 1):\n",
        "\t# \t\tlayer_count[j] = len(sorted)\n",
        "\t# \t\tbreak\n",
        "\t# \ta = self.select_num[:j + 1].sum()\n",
        "\t# \tfor i, val in enumerate(sorted):\n",
        "\t# \t\tflag = True\n",
        "\t# \t\tif flag and val > a:\n",
        "\t# \t\t\tlayer_count[j] += i\n",
        "\t# \t\t\tsorted = sorted[i:]\n",
        "\t# \t\t\tflag = False\n",
        "\t# \t\tif not flag:\n",
        "\t# \t\t\tbreak\n",
        "\t# return layer_count\n",
        "\n",
        "# !pip install ml-collections\n",
        "if __name__ == '__main__':\n",
        "\tstart = time.time()\n",
        "\tconfig = get_b16_config()\n",
        "\t# com = clrEncoder(config,)\n",
        "\t# com.to(device='cuda')\n",
        "\tnet = InterEnsembleLearningTransformer(config).cuda()\n",
        "\t# hidden_state = torch.arange(400*768).reshape(2,200,768)/1.0\n",
        "\tx = torch.rand(4, 3, 448, 448, device='cuda')\n",
        "\ty = net(x)\n",
        "\tprint(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWrOHdM-aX4r",
        "outputId": "61523dd3-d2f4-4a0c-bc04-72b11fa8e953"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for Audio data\n"
      ],
      "metadata": {
        "id": "Sfc-hh7ZlWaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "from transformers import Wav2Vec2Model\n",
        "\n",
        "# from models.modules import *\n",
        "# from models.vit import get_b16_config\n",
        "\n",
        "class InterEnsembleLearningTransformer(nn.Module):\n",
        "\tdef __init__(self, config, img_size=448, num_classes=200, dataset='cub', smooth_value=0.,\n",
        "\t             loss_alpha=0.4, cam=True, dsm=True, fix=True, update_warm=500,\n",
        "\t             vote_perhead=24, total_num=126, assess=False):\n",
        "\t\tsuper(InterEnsembleLearningTransformer, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.smooth_value = smooth_value\n",
        "\t\tself.num_classes = num_classes\n",
        "\t\tself.loss_alpha = loss_alpha\n",
        "\t\tself.cam = cam\n",
        "        # anmol changed 1\n",
        "        # self.embeddings = Embeddings(config, img_size=img_size)\n",
        "\t\tself.embeddings = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "\t\tself.encoder = IELTEncoder(config, update_warm, vote_perhead, dataset, cam, dsm,\n",
        "\t\t                           fix, total_num, assess)\n",
        "\t\tself.head = Linear(config.hidden_size, num_classes)\n",
        "\t\tself.softmax = Softmax(dim=-1)\n",
        "\n",
        "\tdef forward(self, x, labels=None):\n",
        "\t\ttest_mode = False if labels is not None else True\n",
        "        # anmol changed 2\n",
        "\t\tx = self.embeddings(x).last_hidden_state\n",
        "\t\tif self.assess:\n",
        "\t\t\tx, xc, assess_list = self.encoder(x, test_mode)\n",
        "\t\telse:\n",
        "\t\t\tx, xc = self.encoder(x, test_mode)\n",
        "\n",
        "\t\tif self.cam:\n",
        "\t\t\tcomplement_logits = self.head(xc)\n",
        "\t\t\tprobability = self.softmax(complement_logits)\n",
        "\t\t\tweight = self.head.weight\n",
        "\t\t\tassist_logit = probability * (weight.sum(-1))\n",
        "\t\t\tpart_logits = self.head(x) + assist_logit\n",
        "\t\telse:\n",
        "\t\t\tpart_logits = self.head(x)\n",
        "\n",
        "\t\tif self.assess:\n",
        "\t\t\treturn part_logits, assess_list\n",
        "\n",
        "\t\telif test_mode:\n",
        "\t\t\treturn part_logits\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tif self.smooth_value == 0:\n",
        "\t\t\t\tloss_fct = CrossEntropyLoss()\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss_fct = LabelSmoothing(self.smooth_value)\n",
        "\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tloss_p = loss_fct(part_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\t\tloss_c = loss_fct(complement_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\t\talpha = self.loss_alpha\n",
        "\t\t\t\tloss = (1 - alpha) * loss_p + alpha * loss_c\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss = loss_fct(part_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\treturn part_logits, loss\n",
        "\n",
        "\tdef get_eval_data(self):\n",
        "\t\treturn self.encoder.select_num\n",
        "\n",
        "\tdef load_from(self, weights):\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tnn.init.zeros_(self.head.weight)\n",
        "\t\t\tnn.init.zeros_(self.head.bias)\n",
        "\n",
        "\t\t\t# self.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "\t\t\t# self.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "\t\t\t# self.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "\t\t\t# self.encoder.patch_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "\t\t\t# self.encoder.patch_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\t\t\t# self.encoder.clr_encoder.patch_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "\t\t\t# self.encoder.clr_encoder.patch_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "\t\t\t# posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "\t\t\t# posemb_new = self.embeddings.position_embeddings\n",
        "\t\t\t# if posemb.size() == posemb_new.size():\n",
        "\t\t\t# \tself.embeddings.position_embeddings.copy_(posemb)\n",
        "\t\t\t# else:\n",
        "\t\t\t# \tntok_new = posemb_new.size(1)\n",
        "\n",
        "\t\t\t# \tposemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "\t\t\t# \tntok_new -= 1\n",
        "\n",
        "\t\t\t# \tgs_old = int(np.sqrt(len(posemb_grid)))\n",
        "\t\t\t# \tgs_new = int(np.sqrt(ntok_new))\n",
        "\t\t\t# \t# print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "\t\t\t# \tposemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "\t\t\t# \tzoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "\t\t\t# \tposemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "\t\t\t# \tposemb_grid = posemb_grid.reshape((1, gs_new * gs_new, -1))\n",
        "\t\t\t# \tposemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "\t\t\t# \tself.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "\t\t\tfor bname, block in self.encoder.named_children():\n",
        "\t\t\t\tfor uname, unit in block.named_children():\n",
        "\t\t\t\t\tif not bname.startswith('key') and not bname.startswith('clr'):\n",
        "\t\t\t\t\t\tif uname == '12':\n",
        "\t\t\t\t\t\t\tuname = '11'\n",
        "\t\t\t\t\t\tunit.load_from(weights, n_block=uname)\n",
        "\n",
        "class MultiHeadVoting(nn.Module):\n",
        "\tdef __init__(self, config, vote_perhead=24, fix=True):\n",
        "\t\tsuper(MultiHeadVoting, self).__init__()\n",
        "\t\tself.fix = fix\n",
        "\t\tself.num_heads = config.num_heads\n",
        "\t\tself.vote_perhead = vote_perhead\n",
        "\n",
        "        # anmol changed 3\n",
        "\t\t# if self.fix:\n",
        "\t\t# \tself.kernel = torch.tensor([[1, 2, 1],\n",
        "\t\t# \t                            [2, 4, 2],\n",
        "\t\t# \t                            [1, 2, 1]], device='cuda').unsqueeze(0).unsqueeze(0).half()\n",
        "\t\t# \tself.conv = F.conv2d\n",
        "\t\t# else:\n",
        "\t\t# \tself.conv = nn.Conv2d(1, 1, 3, 1, 1)\n",
        "\t\tif self.fix:\n",
        "\t\t\tself.kernel = torch.tensor([1, 2, 1], device='cuda').unsqueeze(0).unsqueeze(0).half()\n",
        "\t\telse:\n",
        "\t\t\tself.kernel = torch.ones(1, 1, 3, device='cuda').half()\n",
        "\n",
        "\tdef forward(self, x, select_num=None, last=False):\n",
        "        # anmol changed 4\n",
        "\t\tB, seq_len = x.shape[0], x.shape[1]  # Adapt for sequence length\n",
        "\t\tselect_num = self.vote_perhead if select_num is None else select_num\n",
        "\t\tcount = torch.zeros((B, seq_len), dtype=torch.int, device='cuda').half()\n",
        "    # anmol changed 5\n",
        "\t\tscore = x[:, :, 0]  # Removed spatial dimension handling for audio\n",
        "\t\t_, select = torch.topk(score, self.vote_perhead, dim=-1)\n",
        "\t\tselect = select.reshape(B, -1)\n",
        "\n",
        "\t\tfor i, b in enumerate(select):\n",
        "\t\t\t# count[i, :] += torch.bincount(b, minlength=seq_len)\n",
        "            # anmol changed 6\n",
        "\t\t\tbincount_result = torch.bincount(b, minlength=seq_len)\n",
        "\t\t\tcount[i, :seq_len] += bincount_result[:seq_len]\n",
        "\n",
        "\t\tif not last:\n",
        "\t\t\tcount = self.enhace_local(count)\n",
        "\n",
        "\t\tpatch_value, patch_idx = torch.sort(count, dim=-1, descending=True)\n",
        "\t\tpatch_idx += 1\n",
        "\t\treturn patch_idx[:, :select_num], count\n",
        "\n",
        "\tdef enhace_local(self, count):\n",
        "        # anmol changed 7\n",
        "\t\tB, seq_len = count.shape[0], count.shape[1]\n",
        "\t\tif self.fix:\n",
        "\t\t\tcount = F.conv1d(count.unsqueeze(1), self.kernel, stride=1, padding=1).reshape(B, -1)\n",
        "\t\telse:\n",
        "\t\t\tcount = F.conv1d(count.unsqueeze(1), self.kernel, stride=1, padding=1).reshape(B, -1)\n",
        "\t\treturn count\n",
        "\n",
        "\n",
        "class IELTEncoder(nn.Module):\n",
        "\tdef __init__(self, config, update_warm=500, vote_perhead=24, dataset='cub',\n",
        "\t             cam=True, dsm=True, fix=True, total_num=126, assess=False):\n",
        "\t\tsuper(IELTEncoder, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.warm_steps = update_warm\n",
        "\t\tself.layer = nn.ModuleList()\n",
        "\t\tself.layer_num = config.num_layers\n",
        "\t\tself.vote_perhead = vote_perhead\n",
        "\t\tself.dataset = dataset\n",
        "\t\tself.cam = cam\n",
        "\t\tself.dsm = dsm\n",
        "\n",
        "\t\tfor _ in range(self.layer_num - 1):\n",
        "\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\n",
        "\t\tif self.dataset == 'dog' or self.dataset == 'nabrids':\n",
        "\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\t\t\tself.clr_layer = self.layer[-1]\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\t\t\t\tself.key_layer = self.layer[-1]\n",
        "\t\telse:\n",
        "\t\t\tself.clr_layer = Block(config)\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tself.key_layer = Block(config)\n",
        "\n",
        "\t\tif self.cam:\n",
        "\t\t\tself.key_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "\t\tself.patch_select = MultiHeadVoting(config, self.vote_perhead, fix)\n",
        "\n",
        "\t\tself.total_num = total_num\n",
        "\t\t## for CUB and NABirds\n",
        "\t\tself.select_rate = torch.tensor([16, 14, 12, 10, 8, 6, 8, 10, 12, 14, 16], device='cuda') / self.total_num\n",
        "\t\t## for Others\n",
        "\t\t# self.select_rate = torch.ones(self.layer_num-1,device='cuda')/(self.layer_num-1)\n",
        "\n",
        "\t\tself.select_num = self.select_rate * self.total_num\n",
        "\t\tself.clr_encoder = CrossLayerRefinement(config, self.clr_layer)\n",
        "\t\tself.count = 0\n",
        "\n",
        "\tdef forward(self, hidden_states, test_mode=False):\n",
        "\t\tif not test_mode:\n",
        "\t\t\tself.count += 1\n",
        "\t\tB, N, C = hidden_states.shape\n",
        "\t\tcomplements = [[] for i in range(B)]\n",
        "\t\tclass_token_list = []\n",
        "\t\tif self.assess:\n",
        "\t\t\tlayer_weights = []\n",
        "\t\t\tlayer_selected = []\n",
        "\t\t\tlayer_score = []\n",
        "\n",
        "\t\tfor t in range(self.layer_num - 1):\n",
        "\t\t\tlayer = self.layer[t]\n",
        "\t\t\tselect_num = torch.round(self.select_num[t]).int()\n",
        "\t\t\thidden_states, weights = layer(hidden_states)\n",
        "\t\t\tselect_idx, select_score = self.patch_select(weights, select_num)\n",
        "\t\t\tfor i in range(B):\n",
        "\t\t\t\tcomplements[i].extend(hidden_states[i, select_idx[i, :]])\n",
        "\t\t\tclass_token_list.append(hidden_states[:, 0].unsqueeze(1))\n",
        "\t\t\tif self.assess:\n",
        "\t\t\t\tlayer_weights.append(weights)\n",
        "\t\t\t\tlayer_score.append(select_score)\n",
        "\t\t\t\tlayer_selected.extend(select_idx)\n",
        "\t\tcls_token = hidden_states[:, 0].unsqueeze(1)\n",
        "\n",
        "\t\tclr, weights = self.clr_encoder(complements, cls_token)\n",
        "\t\tsort_idx, _ = self.patch_select(weights, select_num=24, last=True)\n",
        "\n",
        "\t\tif not test_mode and self.count >= self.warm_steps and self.dsm:\n",
        "\t\t\tlayer_count = self.count_patch(sort_idx)\n",
        "\t\t\tself.update_layer_select(layer_count)\n",
        "\n",
        "\t\tclass_token_list = torch.cat(class_token_list, dim=1)\n",
        "\n",
        "\t\tif not self.cam:\n",
        "\t\t\treturn clr[:, 0], None\n",
        "\t\telse:\n",
        "\t\t\tout = []\n",
        "\t\t\tfor i in range(B):\n",
        "\t\t\t\tout.append(clr[i, sort_idx[i, :]])\n",
        "\t\t\tout = torch.stack(out).squeeze(1)\n",
        "\t\t\tout = torch.cat((cls_token, out), dim=1)\n",
        "\t\t\tout, _ = self.key_layer(out)\n",
        "\t\t\tkey = self.key_norm(out)\n",
        "\n",
        "\t\tif self.assess:\n",
        "\t\t\tassess_list = [layer_weights, layer_selected, layer_score, sort_idx]\n",
        "\t\t\treturn key[:, 0], clr[:, 0], assess_list\n",
        "\t\telse:\n",
        "\t\t\treturn key[:, 0], clr[:, 0]\n",
        "\n",
        "# !pip install ml-collections\n",
        "if __name__ == '__main__':\n",
        "\tstart = time.time()\n",
        "\tconfig = get_b16_config()\n",
        "\tnet = InterEnsembleLearningTransformer(config).cuda()\n",
        "    # anmol changed 8\n",
        "\tx = torch.rand(4, 16000, device='cuda')  # 1-second audio samples at 16kHz\n",
        "\ty = net(x)\n",
        "\tprint(y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262,
          "referenced_widgets": [
            "9826f30327754be3b17b57dcbaf44d89",
            "d1b8bab348b1416bbc25f2968e3f7ae0",
            "dfc07ccc89404cd0ac9794de8efb60e8",
            "5d199c96282c45c2a35ad5e5fa9f3ec5",
            "bfefd3fd073f43608d8efdf7b79851ae",
            "2f63ad22fc8c49daba3eacd9a1d59c3a",
            "9ad493a0046947bc96a4d7e76e17d0b6",
            "f245c685656f4f10bf601a4b29819e07",
            "a0e4f82c570a4f76b23f12589eb17713",
            "0fd27728860a4ab2824d1542b6cde0d4",
            "bc206a769d8646d9937a068b7d3d210b",
            "64f5b6922406427893e2f02a12197693",
            "cb30ac024c21413d9d8ed02939ffdf09",
            "8cea647026b7412f9b81cc4eec0c7b7b",
            "18bc61d866264de18e441e5137b9acbe",
            "d60c103c7f934f8e8205b6c384f3e546",
            "dce9c9a9b62e4cf7919827a6594361b6",
            "0c365dcc9b484f1c8d40ba60cc5219e7",
            "045d088674fd4592a573754c31571df5",
            "673ec031b088447e913548951252c478",
            "1b442cee8c9744299665e94d573ae265",
            "147bc10926924cabb0005e7ce3cf6f66"
          ]
        },
        "id": "fs44df9WbPls",
        "outputId": "8f2ebbb1-dc04-4862-ccb0-9021689cc2cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9826f30327754be3b17b57dcbaf44d89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64f5b6922406427893e2f02a12197693"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import librosa\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start = time.time()\n",
        "    config = get_b16_config()\n",
        "    net = InterEnsembleLearningTransformer(config).cuda()\n",
        "\n",
        "    duration = 10\n",
        "    sampling_rate = 16000\n",
        "\n",
        "    audio = np.random.normal(0, 1, sampling_rate * duration).astype(np.float32)\n",
        "\n",
        "    audio = librosa.util.fix_length(audio, size=sampling_rate * duration)\n",
        "\n",
        "    x = torch.tensor([audio] * 4, device='cuda')\n",
        "\n",
        "    y = net(x)\n",
        "    print(y.shape)\n"
      ],
      "metadata": {
        "id": "U9G3vUrmsWz7",
        "outputId": "c976891a-658e-4d13-ce84-19781e4837b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-9-0a1193217c4d>:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  x = torch.tensor([audio] * 4, device='cuda')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 200])\n"
          ]
        }
      ]
    }
  ]
}