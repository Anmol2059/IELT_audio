{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install pip==24.0\n",
        "# !pip install fairseq\n",
        "!pip install omegaconf\n",
        "!pip install conformer\n",
        "!pip install ml-collections\n",
        "# !wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr2_300m.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrjWODGG4Sx7",
        "outputId": "38b5c110-43b2-4e07-d44f-f7e3d9fbb4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: conformer in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: einops>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from conformer) (0.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from conformer) (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->conformer) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->conformer) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->conformer) (1.3.0)\n",
            "Collecting ml-collections\n",
            "  Using cached ml_collections-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections) (6.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections) (21.6.0)\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temporal Channel Modelling"
      ],
      "metadata": {
        "id": "tqk1WoBCndPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJhBm4_q2fH6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# import fairseq\n",
        "from conformer import ConformerBlock\n",
        "from torch.nn.modules.transformer import _get_clones\n",
        "from torch import Tensor\n",
        "\n",
        "def sinusoidal_embedding(n_channels, dim):\n",
        "    pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
        "                            for p in range(n_channels)])\n",
        "    pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
        "    pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
        "    return pe.unsqueeze(0)\n",
        "\n",
        "class MyConformer(nn.Module):\n",
        "  def __init__(self, emb_size=128, heads=4, ffmult=4, exp_fac=2, kernel_size=16, n_encoders=1):\n",
        "    super(MyConformer, self).__init__()\n",
        "    self.dim_head=int(emb_size/heads)\n",
        "    self.dim=emb_size\n",
        "    self.heads=heads\n",
        "    self.kernel_size=kernel_size\n",
        "    self.n_encoders=n_encoders\n",
        "    self.positional_emb = nn.Parameter(sinusoidal_embedding(10000, emb_size), requires_grad=False)\n",
        "    self.encoder_blocks=_get_clones( ConformerBlock( dim = emb_size, dim_head=self.dim_head, heads= heads,\n",
        "    ff_mult = ffmult, conv_expansion_factor = exp_fac, conv_kernel_size = kernel_size),\n",
        "    n_encoders)\n",
        "    self.class_token = nn.Parameter(torch.rand(1, emb_size))\n",
        "    self.fc5 = nn.Linear(emb_size, 2)\n",
        "\n",
        "  def forward(self, x, device): # x shape [bs, tiempo, frecuencia]\n",
        "    x = x + self.positional_emb[:, :x.size(1), :]\n",
        "    x = torch.stack([torch.vstack((self.class_token, x[i])) for i in range(len(x))])#[bs,1+tiempo,emb_size]\n",
        "    print(f\"Shape after adding class token: {x.shape}\")\n",
        "    list_attn_weight = []\n",
        "    for layer in self.encoder_blocks:\n",
        "            x, attn_weight = layer(x) #[bs,1+tiempo,emb_size]\n",
        "            list_attn_weight.append(attn_weight)\n",
        "    # embedding=x[:,0,:] #[bs, emb_size]\n",
        "    # Debug the shape of x\n",
        "    print(f\"Shape of x before extracting embedding: {x.shape}\")  # Should be [batch_size, 1+time, emb_size]\n",
        "\n",
        "    # Ensure that x has the correct shape (3D) before indexing\n",
        "    if len(x.shape) == 3:\n",
        "      embedding = x[:, 0, :]  # [batch_size, emb_size]\n",
        "    else:\n",
        "      raise ValueError(f\"Unexpected tensor shape: {x.shape}\")\n",
        "\n",
        "    out=self.fc5(embedding) #[bs,2]\n",
        "    return out, list_attn_weight\n",
        "\n",
        "class SSLModel(nn.Module): #W2V\n",
        "    def __init__(self,device):\n",
        "        super(SSLModel, self).__init__()\n",
        "        cp_path = 'xlsr2_300m.pt'   # Change the pre-trained XLSR model path.\n",
        "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
        "        self.model = model[0]\n",
        "        self.device=device\n",
        "        self.out_dim = 1024\n",
        "        return\n",
        "\n",
        "    def extract_feat(self, input_data):\n",
        "        # put the model to GPU if it not there\n",
        "        if next(self.model.parameters()).device != input_data.device \\\n",
        "           or next(self.model.parameters()).dtype != input_data.dtype:\n",
        "            self.model.to(input_data.device, dtype=input_data.dtype)\n",
        "            self.model.train()\n",
        "\n",
        "        # input should be in shape (batch, length)\n",
        "        if input_data.ndim == 3:\n",
        "            input_tmp = input_data[:, :, 0]\n",
        "        else:\n",
        "            input_tmp = input_data\n",
        "\n",
        "        # [batch, length, dim]\n",
        "        emb = self.model(input_tmp, mask=False, features_only=True)['x']\n",
        "        return emb\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, args, device):\n",
        "        super().__init__()\n",
        "        self.device=device\n",
        "        ####\n",
        "        # create network wav2vec 2.0\n",
        "        ####\n",
        "        self.ssl_model = SSLModel(self.device)\n",
        "        self.LL = nn.Linear(1024, args.emb_size)\n",
        "        print('W2V + Conformer')\n",
        "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
        "        self.selu = nn.SELU(inplace=True)\n",
        "        self.conformer=MyConformer(emb_size=args.emb_size, n_encoders=args.num_encoders,\n",
        "        heads=args.heads, kernel_size=args.kernel_size)\n",
        "    def forward(self, x):\n",
        "        #-------pre-trained Wav2vec model fine tunning ------------------------##\n",
        "        x_ssl_feat = self.ssl_model.extract_feat(x.squeeze(-1))\n",
        "        x=self.LL(x_ssl_feat) #(bs,frame_number,feat_out_dim) (bs, 208, 256)\n",
        "        x = x.unsqueeze(dim=1) # add channel #(bs, 1, frame_number, 256)\n",
        "        x = self.first_bn(x)\n",
        "        x = self.selu(x)\n",
        "        x = x.squeeze(dim=1)\n",
        "        out, attn_score =self.conformer(x,self.device)\n",
        "        return out, attn_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "# Parameters\n",
        "duration = 10.0  # seconds\n",
        "sample_rate = 16000\n",
        "frequency = 440.0  #\n",
        "\n",
        "# Time axis\n",
        "t = np.linspace(0.0, duration, int(sample_rate * duration))\n",
        "\n",
        "audio_data = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
        "\n",
        "# Save the audio data as a .flac file in Colab\n",
        "file_path = 'sa.flac'\n",
        "sf.write(file_path, audio_data, sample_rate)\n",
        "\n",
        "print(f\"Audio file saved as {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2jsjhqq_WkL",
        "outputId": "b7f3841f-24b9-44ce-f431-5ea505abb134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio file saved as sa.flac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class MyConformer(nn.Module):\n",
        "    def __init__(self, emb_size=128, heads=4, ffmult=4, exp_fac=2, kernel_size=16, n_encoders=1):\n",
        "        super(MyConformer, self).__init__()\n",
        "        self.dim_head = int(emb_size / heads)\n",
        "        self.dim = emb_size\n",
        "        self.heads = heads\n",
        "        self.kernel_size = kernel_size\n",
        "        self.n_encoders = n_encoders\n",
        "        self.positional_emb = nn.Parameter(sinusoidal_embedding(10000, emb_size), requires_grad=False)\n",
        "        self.encoder_blocks = _get_clones(ConformerBlock(dim=emb_size, dim_head=self.dim_head, heads=heads,\n",
        "                                                         ff_mult=ffmult, conv_expansion_factor=exp_fac,\n",
        "                                                         conv_kernel_size=kernel_size), n_encoders)\n",
        "        self.class_token = nn.Parameter(torch.rand(1, emb_size))\n",
        "        self.fc5 = nn.Linear(emb_size, 2)\n",
        "\n",
        "    def forward(self, x, device):\n",
        "        # Add positional embedding\n",
        "        x = x + self.positional_emb[:, :x.size(1), :]\n",
        "        x = torch.stack([torch.vstack((self.class_token, x[i])) for i in range(len(x))])  # [bs,1+time,emb_size]\n",
        "        print(f\"Shape after adding class token: {x.shape}\")\n",
        "\n",
        "\n",
        "        list_attn_weight = []\n",
        "\n",
        "        # Process through ConformerBlocks\n",
        "        for layer in self.encoder_blocks:\n",
        "\n",
        "            output = layer(x)\n",
        "            x = output\n",
        "\n",
        "\n",
        "\n",
        "        embedding = x[:, 0, :]  # [bs, emb_size]\n",
        "        out = self.fc5(embedding)  # [bs, 2]\n",
        "        return out, list_attn_weight\n",
        "\n",
        "batch_size = 1\n",
        "time_steps = 100\n",
        "emb_size = 128\n",
        "\n",
        "\n",
        "random_input = torch.randn(batch_size, time_steps, emb_size)\n",
        "\n",
        "\n",
        "conformer_model = MyConformer(emb_size=128, heads=4, ffmult=4, exp_fac=2, kernel_size=16, n_encoders=1)\n",
        "\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "\n",
        "conformer_model.eval()\n",
        "with torch.no_grad():\n",
        "    output, attention_weights = conformer_model(random_input, device)\n",
        "\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Output:\", output)\n",
        "print(\"Attention Weights:\", attention_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd9FVnp44R-4",
        "outputId": "a2ba7cce-e5cc-4a39-fd38-ec20a817f083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after adding class token: torch.Size([1, 101, 128])\n",
            "Output shape: torch.Size([1, 2])\n",
            "Output: tensor([[0.9717, 0.1722]])\n",
            "Attention Weights: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image"
      ],
      "metadata": {
        "id": "8MPoVZhImEgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "\n",
        "import copy\n",
        "import math\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "\t\"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "\tif conv:\n",
        "\t\tweights = weights.transpose([3, 2, 0, 1])\n",
        "\treturn torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "\treturn x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "\tdef __init__(self, config):\n",
        "\t\tsuper(Mlp, self).__init__()\n",
        "\t\tself.fc1 = Linear(config.hidden_size, config.mlp_dim)\n",
        "\t\tself.fc2 = Linear(config.mlp_dim, config.hidden_size)\n",
        "\t\tself.act_fn = ACT2FN[\"gelu\"]\n",
        "\t\tself.dropout = Dropout(config.dropout_rate)\n",
        "\n",
        "\t\tself._init_weights()\n",
        "\n",
        "\tdef _init_weights(self):\n",
        "\t\tnn.init.xavier_uniform_(self.fc1.weight)\n",
        "\t\tnn.init.xavier_uniform_(self.fc2.weight)\n",
        "\t\tnn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "\t\tnn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = self.act_fn(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\tx = self.fc2(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "\t\"\"\"Construct the embeddings from patch, position embeddings.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, config, img_size, in_channels=3):\n",
        "\t\tsuper(Embeddings, self).__init__()\n",
        "\t\timg_size = _pair(img_size)\n",
        "\n",
        "\t\tpatch_size = _pair(config.patches)\n",
        "\t\tn_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "\t\tself.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "\t\t                               out_channels=config.hidden_size,\n",
        "\t\t                               kernel_size=patch_size,\n",
        "\t\t                               stride=patch_size)\n",
        "\t\tself.position_embeddings = nn.Parameter(torch.zeros(1, n_patches + 1, config.hidden_size))\n",
        "\t\tself.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "\t\tself.dropout = Dropout(config.dropout_rate)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tB = x.shape[0]\n",
        "\t\tcls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "\t\tx = self.patch_embeddings(x)\n",
        "\t\tx = x.flatten(2)\n",
        "\t\tx = x.transpose(-1, -2)\n",
        "\t\tx = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "\t\tembeddings = x + self.position_embeddings\n",
        "\t\tembeddings = self.dropout(embeddings)\n",
        "\t\treturn embeddings\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\tdef __init__(self, config):\n",
        "\t\tsuper(Encoder, self).__init__()\n",
        "\t\tself.layer = nn.ModuleList()\n",
        "\t\t# for _ in range(config.num_layers):\n",
        "\t\tfor _ in range(config.num_layers + 1):\n",
        "\t\t\tlayer = Block(config)\n",
        "\t\t\tself.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "\tdef forward(self, hidden_states):\n",
        "\t\t# attmap = []\n",
        "\t\tfor layer in self.layer:\n",
        "\t\t\thidden_states, weights = layer(hidden_states)\n",
        "\t\t# print(weights.shape)\n",
        "\t\t# attmap.append(weights)\n",
        "\t\treturn hidden_states\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\tdef __init__(self, config, img_size):\n",
        "\t\tsuper(Transformer, self).__init__()\n",
        "\t\tself.embeddings = Embeddings(config, img_size=img_size)\n",
        "\t\tself.encoder = Encoder(config)\n",
        "\n",
        "\tdef forward(self, input_ids):\n",
        "\t\tembedding_output = self.embeddings(input_ids)\n",
        "\t\tpart_encoded = self.encoder(embedding_output)\n",
        "\t\treturn part_encoded\n",
        "\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "\t\"\"\"\n",
        "\tNLL loss with label smoothing.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, smoothing=0.0):\n",
        "\t\t\"\"\"\n",
        "\t\tConstructor for the LabelSmoothing module.\n",
        "\t\tparam smoothing: label smoothing factor\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(LabelSmoothing, self).__init__()\n",
        "\t\tself.confidence = 1.0 - smoothing\n",
        "\t\tself.smoothing = smoothing\n",
        "\n",
        "\tdef forward(self, x, target):\n",
        "\t\tlogprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
        "\t\tnll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "\t\tnll_loss = nll_loss.squeeze(1)\n",
        "\t\tsmooth_loss = -logprobs.mean(dim=-1)\n",
        "\t\tloss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "\t\treturn loss.mean()\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\tdef __init__(self, config, assess=False):\n",
        "\t\tsuper(Attention, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.num_attention_heads = config.num_heads\n",
        "\t\tself.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "\t\tself.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "\t\tself.query = Linear(config.hidden_size, self.all_head_size)\n",
        "\t\tself.key = Linear(config.hidden_size, self.all_head_size)\n",
        "\t\tself.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "\t\tself.out = Linear(config.hidden_size, config.hidden_size)\n",
        "\t\tself.attn_dropout = Dropout(config.att_dropout)\n",
        "\t\tself.proj_dropout = Dropout(config.att_dropout)\n",
        "\n",
        "\t\tself.softmax = Softmax(dim=-1)\n",
        "\n",
        "\tdef transpose_for_scores(self, x):\n",
        "\t\tnew_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "\t\tx = x.view(*new_x_shape)\n",
        "\t\treturn x.permute(0, 2, 1, 3)\n",
        "\n",
        "\tdef forward(self, hidden_states):\n",
        "\t\tmixed_query_layer = self.query(hidden_states)\n",
        "\t\tmixed_key_layer = self.key(hidden_states)\n",
        "\t\tmixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "\t\tquery_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\t\tkey_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "\t\tvalue_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "\t\tattention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\t\tattention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\t\tattention_probs = self.softmax(attention_scores)\n",
        "\t\tweights = attention_probs\n",
        "\t\tattention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "\t\tcontext_layer = torch.matmul(attention_probs, value_layer)\n",
        "\t\tcontext_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "\t\tnew_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "\t\tcontext_layer = context_layer.view(*new_context_layer_shape)\n",
        "\t\tattention_output = self.out(context_layer)\n",
        "\t\tattention_output = self.proj_dropout(attention_output)\n",
        "\t\tif self.assess:\n",
        "\t\t\treturn attention_output, weights, attention_scores\n",
        "\t\telse:\n",
        "\t\t\treturn attention_output, weights\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\tdef __init__(self, config, assess=False):\n",
        "\t\tsuper(Block, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.hidden_size = config.hidden_size\n",
        "\t\tself.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\t\tself.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\t\tself.ffn = Mlp(config)\n",
        "\t\tself.attn = Attention(config, self.assess)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\th = x\n",
        "\t\tx = self.attention_norm(x)\n",
        "\t\tif self.assess:\n",
        "\t\t\tx, weights, score = self.attn(x)\n",
        "\t\telse:\n",
        "\t\t\tx, weights = self.attn(x)\n",
        "\t\tx = x + h\n",
        "\n",
        "\t\th = x\n",
        "\t\tx = self.ffn_norm(x)\n",
        "\t\tx = self.ffn(x)\n",
        "\t\tx = x + h\n",
        "\t\treturn x, weights\n",
        "\n",
        "\tdef load_from(self, weights, n_block):\n",
        "\t\tROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tquery_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size,\n",
        "\t\t\t                                                                       self.hidden_size).t()\n",
        "\t\t\tkey_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\t\t\tvalue_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size,\n",
        "\t\t\t                                                                       self.hidden_size).t()\n",
        "\t\t\tout_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size,\n",
        "\t\t\t                                                                       self.hidden_size).t()\n",
        "\n",
        "\t\t\tquery_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "\t\t\tkey_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "\t\t\tvalue_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "\t\t\tout_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "\t\t\tself.attn.query.weight.copy_(query_weight)\n",
        "\t\t\tself.attn.key.weight.copy_(key_weight)\n",
        "\t\t\tself.attn.value.weight.copy_(value_weight)\n",
        "\t\t\tself.attn.out.weight.copy_(out_weight)\n",
        "\t\t\tself.attn.query.bias.copy_(query_bias)\n",
        "\t\t\tself.attn.key.bias.copy_(key_bias)\n",
        "\t\t\tself.attn.value.bias.copy_(value_bias)\n",
        "\t\t\tself.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "\t\t\tmlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "\t\t\tmlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "\t\t\tmlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "\t\t\tmlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "\t\t\tself.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "\t\t\tself.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "\t\t\tself.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "\t\t\tself.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "\t\t\tself.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "\t\t\tself.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "\t\t\tself.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "\t\t\tself.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "# from core.vit import *\n",
        "# config = get_b16_config()\n",
        "import ml_collections\n",
        "def get_b16_config():\n",
        "\t\"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "\tconfig = ml_collections.ConfigDict()\n",
        "\tconfig.patches = (16, 16)\n",
        "\tconfig.hidden_size = 768\n",
        "\tconfig.mlp_dim = 3072\n",
        "\tconfig.num_heads = 12\n",
        "\tconfig.num_layers = 12\n",
        "\tconfig.att_dropout = 0.0\n",
        "\tconfig.dropout_rate = 0.1\n",
        "\tconfig.classifier = 'token'\n",
        "\treturn config\n"
      ],
      "metadata": {
        "id": "1qlfUM4qmJOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn.functional as F\n",
        "# from models.modules import *\n",
        "# from models.vit import get_b16_config\n",
        "\n",
        "\n",
        "class InterEnsembleLearningTransformer(nn.Module):\n",
        "\tdef __init__(self, config, img_size=448, num_classes=2, dataset='cub', smooth_value=0.,\n",
        "\t             loss_alpha=0.4, cam=True, dsm=True, fix=True, update_warm=500,\n",
        "\t             vote_perhead=24, total_num=126, assess=False):\n",
        "\t\tsuper(InterEnsembleLearningTransformer, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.smooth_value = smooth_value\n",
        "\t\tself.num_classes = num_classes\n",
        "\t\tself.loss_alpha = loss_alpha\n",
        "\t\tself.cam = cam\n",
        "\n",
        "\t\tself.embeddings = Embeddings(config, img_size=img_size)\n",
        "\t\tself.encoder = IELTEncoder(config, update_warm, vote_perhead, dataset, cam, dsm,\n",
        "\t\t                           fix, total_num, assess)\n",
        "\t\tself.head = Linear(config.hidden_size, num_classes)\n",
        "\t\tself.softmax = Softmax(dim=-1)\n",
        "\n",
        "\n",
        "\tdef forward(self, x, labels=None):\n",
        "\t\ttest_mode = False if labels is not None else True\n",
        "\t\tx = self.embeddings(x)\n",
        "\t\tif self.assess:\n",
        "\t\t\tx, xc, assess_list = self.encoder(x, test_mode)\n",
        "\t\telse:\n",
        "\t\t\tx, xc = self.encoder(x, test_mode)\n",
        "\n",
        "\t\tif self.cam:\n",
        "\t\t\tcomplement_logits = self.head(xc)\n",
        "\t\t\tprobability = self.softmax(complement_logits)\n",
        "\t\t\tweight = self.head.weight\n",
        "\t\t\tassist_logit = probability * (weight.sum(-1))\n",
        "\t\t\tpart_logits = self.head(x) + assist_logit\n",
        "\t\telse:\n",
        "\t\t\tpart_logits = self.head(x)\n",
        "\n",
        "\t\tif self.assess:\n",
        "\t\t\treturn part_logits, assess_list\n",
        "\n",
        "\t\telif test_mode:\n",
        "\t\t\treturn part_logits\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tif self.smooth_value == 0:\n",
        "\t\t\t\tloss_fct = CrossEntropyLoss()\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss_fct = LabelSmoothing(self.smooth_value)\n",
        "\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tloss_p = loss_fct(part_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\t\tloss_c = loss_fct(complement_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\t\talpha = self.loss_alpha\n",
        "\t\t\t\tloss = (1 - alpha) * loss_p + alpha * loss_c\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss = loss_fct(part_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "\t\t\treturn part_logits, loss\n",
        "\n",
        "\tdef get_eval_data(self):\n",
        "\t\treturn self.encoder.select_num\n",
        "\n",
        "\tdef load_from(self, weights):\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tnn.init.zeros_(self.head.weight)\n",
        "\t\t\tnn.init.zeros_(self.head.bias)\n",
        "\n",
        "\t\t\tself.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "\t\t\tself.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "\t\t\tself.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "\t\t\t# self.encoder.patch_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "\t\t\t# self.encoder.patch_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\t\t\t# self.encoder.clr_encoder.patch_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "\t\t\t# self.encoder.clr_encoder.patch_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "\t\t\tposemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "\t\t\tposemb_new = self.embeddings.position_embeddings\n",
        "\t\t\tif posemb.size() == posemb_new.size():\n",
        "\t\t\t\tself.embeddings.position_embeddings.copy_(posemb)\n",
        "\t\t\telse:\n",
        "\t\t\t\tntok_new = posemb_new.size(1)\n",
        "\n",
        "\t\t\t\tposemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "\t\t\t\tntok_new -= 1\n",
        "\n",
        "\t\t\t\tgs_old = int(np.sqrt(len(posemb_grid)))\n",
        "\t\t\t\tgs_new = int(np.sqrt(ntok_new))\n",
        "\t\t\t\t# print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "\t\t\t\tposemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "\t\t\t\tzoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "\t\t\t\tposemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "\t\t\t\tposemb_grid = posemb_grid.reshape((1, gs_new * gs_new, -1))\n",
        "\t\t\t\tposemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "\t\t\t\tself.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "\t\t\tfor bname, block in self.encoder.named_children():\n",
        "\t\t\t\tfor uname, unit in block.named_children():\n",
        "\t\t\t\t\tif not bname.startswith('key') and not bname.startswith('clr'):\n",
        "\t\t\t\t\t\tif uname == '12':\n",
        "\t\t\t\t\t\t\tuname = '11'\n",
        "\t\t\t\t\t\tunit.load_from(weights, n_block=uname)\n",
        "\n",
        "\n",
        "class MultiHeadVoting(nn.Module):\n",
        "\tdef __init__(self, config, vote_perhead=24, fix=True):\n",
        "\t\tsuper(MultiHeadVoting, self).__init__()\n",
        "\t\tself.fix = fix\n",
        "\t\tself.num_heads = config.num_heads\n",
        "\t\tself.vote_perhead = vote_perhead\n",
        "\n",
        "\t\tif self.fix:\n",
        "\t\t\tself.kernel = torch.tensor([[1, 2, 1],\n",
        "\t\t\t                            [2, 4, 2],\n",
        "\t\t\t                            [1, 2, 1]], device='cuda').unsqueeze(0).unsqueeze(0).half()\n",
        "\t\t\tself.conv = F.conv2d\n",
        "\t\telse:\n",
        "\t\t\tself.conv = nn.Conv2d(1, 1, 3, 1, 1)\n",
        "\n",
        "\tdef forward(self, x, select_num=None, last=False):\n",
        "\t\tB, patch_num = x.shape[0], x.shape[3] - 1\n",
        "\t\tselect_num = self.vote_perhead if select_num is None else select_num\n",
        "\t\tcount = torch.zeros((B, patch_num), dtype=torch.int, device='cuda').half()\n",
        "\t\tscore = x[:, :, 0, 1:]\n",
        "\t\t_, select = torch.topk(score, self.vote_perhead, dim=-1)\n",
        "\t\tselect = select.reshape(B, -1)\n",
        "\n",
        "\t\tfor i, b in enumerate(select):\n",
        "\t\t\tcount[i, :] += torch.bincount(b, minlength=patch_num)\n",
        "\n",
        "\t\tif not last:\n",
        "\t\t\tcount = self.enhace_local(count)\n",
        "\t\t\tpass\n",
        "\n",
        "\t\tpatch_value, patch_idx = torch.sort(count, dim=-1, descending=True)\n",
        "\t\tpatch_idx += 1\n",
        "\t\treturn patch_idx[:, :select_num], count\n",
        "\n",
        "\tdef enhace_local(self, count):\n",
        "\t\tB, H = count.shape[0], math.ceil(math.sqrt(count.shape[1]))\n",
        "\t\tcount = count.reshape(B, H, H)\n",
        "\t\tif self.fix:\n",
        "\t\t\tcount = self.conv(count.unsqueeze(1), self.kernel, stride=1, padding=1).reshape(B, -1)\n",
        "\t\telse:\n",
        "\t\t\tcount = self.conv(count.unsqueeze(1)).reshape(B, -1)\n",
        "\t\treturn count\n",
        "\n",
        "\n",
        "class CrossLayerRefinement(nn.Module):\n",
        "\tdef __init__(self, config, clr_layer):\n",
        "\t\tsuper(CrossLayerRefinement, self).__init__()\n",
        "\t\tself.clr_layer = clr_layer\n",
        "\t\tself.clr_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "\tdef forward(self, x, cls):\n",
        "\t\tout = [torch.stack(token) for token in x]\n",
        "\t\tout = torch.stack(out).squeeze(1)\n",
        "\t\tout = torch.cat((cls, out), dim=1)\n",
        "\t\tout, weights = self.clr_layer(out)\n",
        "\t\tout = self.clr_norm(out)\n",
        "\t\treturn out, weights\n",
        "\n",
        "\n",
        "class IELTEncoder(nn.Module):\n",
        "\tdef __init__(self, config, update_warm=500, vote_perhead=24, dataset='cub',\n",
        "\t             cam=True, dsm=True, fix=True, total_num=126, assess=False):\n",
        "\t\tsuper(IELTEncoder, self).__init__()\n",
        "\t\tself.assess = assess\n",
        "\t\tself.warm_steps = update_warm\n",
        "\t\tself.layer = nn.ModuleList()\n",
        "\t\tself.layer_num = config.num_layers\n",
        "\t\tself.vote_perhead = vote_perhead\n",
        "\t\tself.dataset = dataset\n",
        "\t\tself.cam = cam\n",
        "\t\tself.dsm = dsm\n",
        "\n",
        "\t\tfor _ in range(self.layer_num - 1):\n",
        "\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\n",
        "\t\tif self.dataset == 'dog' or self.dataset == 'nabrids':\n",
        "\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\t\t\tself.clr_layer = self.layer[-1]\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tself.layer.append(Block(config, assess=self.assess))\n",
        "\t\t\t\tself.key_layer = self.layer[-1]\n",
        "\t\telse:\n",
        "\t\t\tself.clr_layer = Block(config)\n",
        "\t\t\tif self.cam:\n",
        "\t\t\t\tself.key_layer = Block(config)\n",
        "\n",
        "\t\tif self.cam:\n",
        "\t\t\tself.key_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "\t\tself.patch_select = MultiHeadVoting(config, self.vote_perhead, fix)\n",
        "\n",
        "\t\tself.total_num = total_num\n",
        "\t\t## for CUB and NABirds\n",
        "\t\tself.select_rate = torch.tensor([16, 14, 12, 10, 8, 6, 8, 10, 12, 14, 16], device='cuda') / self.total_num\n",
        "\t\t## for Others\n",
        "\t\t# self.select_rate = torch.ones(self.layer_num-1,device='cuda')/(self.layer_num-1)\n",
        "\n",
        "\t\tself.select_num = self.select_rate * self.total_num\n",
        "\t\tself.clr_encoder = CrossLayerRefinement(config, self.clr_layer)\n",
        "\t\tself.count = 0\n",
        "\n",
        "\tdef forward(self, hidden_states, test_mode=False):\n",
        "\t\tif not test_mode:\n",
        "\t\t\tself.count += 1\n",
        "\t\tB, N, C = hidden_states.shape\n",
        "\t\tcomplements = [[] for i in range(B)]\n",
        "\t\tclass_token_list = []\n",
        "\t\tif self.assess:\n",
        "\t\t\tlayer_weights = []\n",
        "\t\t\tlayer_selected = []\n",
        "\t\t\tlayer_score = []\n",
        "\t\telse:\n",
        "\t\t\tpass\n",
        "\n",
        "\t\tfor t in range(self.layer_num - 1):\n",
        "\t\t\tlayer = self.layer[t]\n",
        "\t\t\tselect_num = torch.round(self.select_num[t]).int()\n",
        "\t\t\thidden_states, weights = layer(hidden_states)\n",
        "\t\t\tselect_idx, select_score = self.patch_select(weights, select_num)\n",
        "\t\t\tfor i in range(B):\n",
        "\t\t\t\tcomplements[i].extend(hidden_states[i, select_idx[i, :]])\n",
        "\t\t\tclass_token_list.append(hidden_states[:, 0].unsqueeze(1))\n",
        "\t\t\tif self.assess:\n",
        "\t\t\t\tlayer_weights.append(weights)\n",
        "\t\t\t\tlayer_score.append(select_score)\n",
        "\t\t\t\tlayer_selected.extend(select_idx)\n",
        "\t\tcls_token = hidden_states[:, 0].unsqueeze(1)\n",
        "\n",
        "\t\tclr, weights = self.clr_encoder(complements, cls_token)\n",
        "\t\tsort_idx, _ = self.patch_select(weights, select_num=24, last=True)\n",
        "\n",
        "\t\tif not test_mode and self.count >= self.warm_steps and self.dsm:\n",
        "\t\t\t# if not test_mode and self.count >= 500 and self.dsm:\n",
        "\t\t\tlayer_count = self.count_patch(sort_idx)\n",
        "\t\t\tself.update_layer_select(layer_count)\n",
        "\n",
        "\t\tclass_token_list = torch.cat(class_token_list, dim=1)\n",
        "\n",
        "\t\tif not self.cam:\n",
        "\t\t\treturn clr[:, 0], None\n",
        "\t\telse:\n",
        "\t\t\tout = []\n",
        "\t\t\tfor i in range(B):\n",
        "\t\t\t\tout.append(clr[i, sort_idx[i, :]])\n",
        "\t\t\tout = torch.stack(out).squeeze(1)\n",
        "\t\t\tout = torch.cat((cls_token, out), dim=1)\n",
        "\t\t\tout, _ = self.key_layer(out)\n",
        "\t\t\tkey = self.key_norm(out)\n",
        "\n",
        "\t\tif self.assess:\n",
        "\t\t\tassess_list = [layer_weights, layer_selected, layer_score, sort_idx]\n",
        "\t\t\treturn key[:, 0], clr[:, 0], assess_list\n",
        "\t\telse:\n",
        "\n",
        "\t\t\t# fused = torch.cat((class_token_list, clr[:, 0].unsqueeze(1)), dim=1)\n",
        "\t\t\t# clr[:, 0] = fused.mean(1)\n",
        "\t\t\treturn key[:, 0], clr[:, 0]\n",
        "\n",
        "\tdef update_layer_select(self, layer_count):\n",
        "\t\talpha = 1e-3  # if self.dataset != 'dog' and self.dataset == 'nabirds' else 1e-4\n",
        "\t\tnew_rate = layer_count / layer_count.sum()\n",
        "\n",
        "\t\tself.select_rate = self.select_rate * (1 - alpha) + alpha * new_rate\n",
        "\t\tself.select_rate /= self.select_rate.sum()\n",
        "\t\tself.select_num = self.select_rate * self.total_num\n",
        "\n",
        "\tdef count_patch(self, sort_idx):\n",
        "\t\tlayer_count = torch.cumsum(self.select_num, dim=-1)\n",
        "\t\tsort_idx = (sort_idx - 1).reshape(-1)\n",
        "\t\tfor i in range(self.layer_num - 1):\n",
        "\t\t\tmask = (sort_idx < layer_count[i])\n",
        "\t\t\tlayer_count[i] = mask.sum()\n",
        "\t\tcum_count = torch.cat((torch.tensor([0], device='cuda'), layer_count[:-1]))\n",
        "\t\tlayer_count -= cum_count\n",
        "\t\treturn layer_count.int()\n",
        "\n",
        "\t## Old Implementation\n",
        "\t# layer_count = torch.zeros(self.layer_num, device='cuda').int()\n",
        "\t# sort_idx = (sort_idx - 1).reshape(-1)\n",
        "\t# sorted, _ = torch.sort(sort_idx)\n",
        "\t# for j in range(self.layer_num):\n",
        "\t# \tif j == (self.layer_num - 1):\n",
        "\t# \t\tlayer_count[j] = len(sorted)\n",
        "\t# \t\tbreak\n",
        "\t# \ta = self.select_num[:j + 1].sum()\n",
        "\t# \tfor i, val in enumerate(sorted):\n",
        "\t# \t\tflag = True\n",
        "\t# \t\tif flag and val > a:\n",
        "\t# \t\t\tlayer_count[j] += i\n",
        "\t# \t\t\tsorted = sorted[i:]\n",
        "\t# \t\t\tflag = False\n",
        "\t# \t\tif not flag:\n",
        "\t# \t\t\tbreak\n",
        "\t# return layer_count\n",
        "\n",
        "!pip install ml-collections\n",
        "if __name__ == '__main__':\n",
        "\tstart = time.time()\n",
        "\tconfig = get_b16_config()\n",
        "\t# com = clrEncoder(config,)\n",
        "\t# com.to(device='cuda')\n",
        "\tnet = InterEnsembleLearningTransformer(config).cuda()\n",
        "\t# hidden_state = torch.arange(400*768).reshape(2,200,768)/1.0\n",
        "\tx = torch.rand(1, 3, 448, 448, device='cuda')\n",
        "\ty = net(x)\n",
        "\tprint(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXZneLW8l8iz",
        "outputId": "920709f0-5dd4-4b8b-f4d3-387314aca576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ml-collections in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections) (6.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections) (21.6.0)\n",
            "torch.Size([4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio with this ensemble learning"
      ],
      "metadata": {
        "id": "8V4Gxwwyjdgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conformer.py from Tuan"
      ],
      "metadata": {
        "id": "5vZ2UC_gxquO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from einops import rearrange\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "def calc_same_padding(kernel_size):\n",
        "    pad = kernel_size // 2\n",
        "    return (pad, pad - (kernel_size + 1) % 2)\n",
        "\n",
        "# helper classes\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * x.sigmoid()\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, gate = x.chunk(2, dim=self.dim)\n",
        "        return out * gate.sigmoid()\n",
        "\n",
        "class DepthWiseConv1d(nn.Module):\n",
        "    def __init__(self, chan_in, chan_out, kernel_size, padding):\n",
        "        super().__init__()\n",
        "        self.padding = padding\n",
        "        self.conv = nn.Conv1d(chan_in, chan_out, kernel_size, groups = chan_in)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, self.padding)\n",
        "        return self.conv(x)\n",
        "\n",
        "# attention, feedforward, and conv module\n",
        "\n",
        "class Scale(nn.Module):\n",
        "    def __init__(self, scale, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) * self.scale\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x, **kwargs)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    # Head Token attention: https://arxiv.org/pdf/2210.05958.pdf\n",
        "    def __init__(self, dim, heads=8, dim_head=64, qkv_bias=False, dropout=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, inner_dim * 3, bias=qkv_bias)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(inner_dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        self.act = nn.GELU()\n",
        "        self.ht_proj = nn.Linear(dim_head, dim,bias=True)\n",
        "        self.ht_norm = nn.LayerNorm(dim_head)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_heads, dim))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # head token\n",
        "        head_pos = self.pos_embed.expand(x.shape[0], -1, -1)\n",
        "        x_ = x.reshape(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        x_ = x_.mean(dim=2)  # now the shape is [B, h, 1, d//h]\n",
        "        x_ = self.ht_proj(x_).reshape(B, -1, self.num_heads, C // self.num_heads)\n",
        "        x_ = self.act(self.ht_norm(x_)).flatten(2)\n",
        "        x_ = x_ + head_pos\n",
        "        x = torch.cat([x, x_], dim=1)\n",
        "\n",
        "        # normal mhsa\n",
        "        qkv = self.qkv(x).reshape(B, N+self.num_heads, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        # attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N+self.num_heads, C)\n",
        "        x = self.proj(x)\n",
        "\n",
        "        # merge head tokens into cls token\n",
        "        cls, patch, ht = torch.split(x, [1, N-1, self.num_heads], dim=1)\n",
        "        cls = cls + torch.mean(ht, dim=1, keepdim=True) + torch.mean(patch, dim=1, keepdim=True)\n",
        "        x = torch.cat([cls, patch], dim=1)\n",
        "\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        mult = 4,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            Swish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ConformerConvModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        causal = False,\n",
        "        expansion_factor = 2,\n",
        "        kernel_size = 31,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        inner_dim = dim * expansion_factor\n",
        "        padding = calc_same_padding(kernel_size) if not causal else (kernel_size - 1, 0)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            Rearrange('b n c -> b c n'),\n",
        "            nn.Conv1d(dim, inner_dim * 2, 1),\n",
        "            GLU(dim=1),\n",
        "            DepthWiseConv1d(inner_dim, inner_dim, kernel_size = kernel_size, padding = padding),\n",
        "            nn.BatchNorm1d(inner_dim) if not causal else nn.Identity(),\n",
        "            Swish(),\n",
        "            nn.Conv1d(inner_dim, dim, 1),\n",
        "            Rearrange('b c n -> b n c'),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Conformer Block\n",
        "\n",
        "class ConformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        ff_mult = 4,\n",
        "        conv_expansion_factor = 2,\n",
        "        conv_kernel_size = 31,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.,\n",
        "        conv_dropout = 0.,\n",
        "        conv_causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ff1 = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
        "        self.attn = Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)\n",
        "        self.conv = ConformerConvModule(dim = dim, causal = conv_causal, expansion_factor = conv_expansion_factor, kernel_size = conv_kernel_size, dropout = conv_dropout)\n",
        "        self.ff2 = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
        "\n",
        "        self.attn = PreNorm(dim, self.attn)\n",
        "        self.ff1 = Scale(0.5, PreNorm(dim, self.ff1))\n",
        "        self.ff2 = Scale(0.5, PreNorm(dim, self.ff2))\n",
        "\n",
        "        self.post_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        x = self.ff1(x) + x\n",
        "        attn_x, attn_weight = self.attn(x, mask = mask)\n",
        "        x = attn_x + x\n",
        "        x = self.conv(x) + x\n",
        "        x = self.ff2(x) + x\n",
        "        x = self.post_norm(x)\n",
        "        return x, attn_weight\n",
        "\n",
        "# Conformer\n",
        "\n",
        "class Conformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        *,\n",
        "        depth,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        ff_mult = 4,\n",
        "        conv_expansion_factor = 2,\n",
        "        conv_kernel_size = 31,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.,\n",
        "        conv_dropout = 0.,\n",
        "        conv_causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(ConformerBlock(\n",
        "                dim = dim,\n",
        "                dim_head = dim_head,\n",
        "                heads = heads,\n",
        "                ff_mult = ff_mult,\n",
        "                conv_expansion_factor = conv_expansion_factor,\n",
        "                conv_kernel_size = conv_kernel_size,\n",
        "                conv_causal = conv_causal\n",
        "\n",
        "            ))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        for block in self.layers:\n",
        "            x = block(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "VKDmaOIkjiGe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AudioInterEnsembleLearningTransformer(nn.Module):\n",
        "    def __init__(self, config, num_classes=2, emb_size=128, timesteps=100, vote_perhead=24, total_num=126, assess=False):\n",
        "        super(AudioInterEnsembleLearningTransformer, self).__init__()\n",
        "        self.assess = assess\n",
        "        self.num_classes = num_classes\n",
        "        self.vote_perhead = vote_perhead\n",
        "        self.total_num = total_num\n",
        "\n",
        "\n",
        "        self.positional_emb = nn.Parameter(self.sinusoidal_embedding(timesteps, emb_size), requires_grad=False)\n",
        "\n",
        "\n",
        "        self.encoder = IELTEncoder(config, vote_perhead=vote_perhead, total_num=total_num, assess=assess)\n",
        "\n",
        "\n",
        "        self.head = nn.Linear(config.hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        test_mode = False if labels is not None else True\n",
        "\n",
        "        print(\"Input shape:\", x.shape)\n",
        "\n",
        "        self.positional_emb = self.positional_emb.to(x.device)\n",
        "\n",
        "        print(\"Positional embedding shape:\", self.positional_emb[:x.size(1), :].unsqueeze(0).shape)\n",
        "\n",
        "        x = x + self.positional_emb[:x.size(1), :].unsqueeze(0)\n",
        "\n",
        "        print(\"Shape after positional embedding:\", x.shape)\n",
        "\n",
        "        if self.assess:\n",
        "            x, xc, assess_list = self.encoder(x, test_mode)\n",
        "        else:\n",
        "            x, xc = self.encoder(x, test_mode)\n",
        "\n",
        "        print(\"Encoder output shape:\", xc.shape)\n",
        "\n",
        "        complement_logits = self.head(xc)\n",
        "\n",
        "        print(\"Shape after linear (head) layer:\", complement_logits.shape)\n",
        "\n",
        "        probability = self.softmax(complement_logits)\n",
        "        weight = self.head.weight\n",
        "        assist_logit = probability * weight.sum(-1)\n",
        "\n",
        "        print(\"Shape of assist_logit:\", assist_logit.shape)\n",
        "\n",
        "        part_logits = self.head(x) + assist_logit\n",
        "\n",
        "        print(\"Final part_logits shape:\", part_logits.shape)\n",
        "\n",
        "        if self.assess:\n",
        "            return part_logits, assess_list\n",
        "        elif test_mode:\n",
        "            return part_logits\n",
        "        else:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(part_logits.view(-1, self.num_classes), labels.view(-1))\n",
        "            return part_logits, loss\n",
        "\n",
        "\n",
        "    def sinusoidal_embedding(self, n_position, d_model):\n",
        "        # Standard sinusoidal embedding used in transformers\n",
        "        position = torch.arange(0, n_position, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe = torch.zeros(n_position, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe\n",
        "\n",
        "\n",
        "class IELTEncoder(nn.Module):\n",
        "    def __init__(self, config, vote_perhead=24, total_num=126, assess=False):\n",
        "        super(IELTEncoder, self).__init__()\n",
        "        self.assess = assess\n",
        "        self.layer_num = config.num_layers\n",
        "        self.vote_perhead = vote_perhead\n",
        "\n",
        "        # I Replaced Block with ConformerBlock\n",
        "        self.layer = nn.ModuleList([\n",
        "            ConformerBlock(dim=config.hidden_size, dim_head=config.hidden_size // config.num_heads, heads=config.num_heads)\n",
        "            for _ in range(self.layer_num - 1)\n",
        "        ])\n",
        "\n",
        "        self.clr_layer = ConformerBlock(dim=config.hidden_size, dim_head=config.hidden_size // config.num_heads, heads=config.num_heads)\n",
        "        self.clr_encoder = CrossLayerRefinement(config, self.clr_layer)\n",
        "        self.patch_select = MultiHeadVoting(config, self.vote_perhead)\n",
        "\n",
        "    def forward(self, hidden_states, test_mode=False):\n",
        "        B, N, C = hidden_states.shape\n",
        "        complements = [[] for _ in range(B)]\n",
        "        class_token_list = []\n",
        "\n",
        "        for t in range(self.layer_num - 1):\n",
        "            layer = self.layer[t]\n",
        "            hidden_states, weights = layer(hidden_states)  # Using ConformerBlock\n",
        "            select_idx, select_score = self.patch_select(weights)\n",
        "            for i in range(B):\n",
        "                complements[i].extend(hidden_states[i, select_idx[i, :]])\n",
        "            class_token_list.append(hidden_states[:, 0].unsqueeze(1))\n",
        "\n",
        "        cls_token = hidden_states[:, 0].unsqueeze(1)\n",
        "        clr, weights = self.clr_encoder(complements, cls_token)\n",
        "\n",
        "        return clr[:, 0], weights\n",
        "\n",
        "\n",
        "class MultiHeadVoting(nn.Module):\n",
        "    def __init__(self, config, vote_perhead=24, fix=True):\n",
        "        super(MultiHeadVoting, self).__init__()\n",
        "        self.fix = fix\n",
        "        self.num_heads = config.num_heads\n",
        "        self.vote_perhead = vote_perhead\n",
        "\n",
        "        if self.fix:\n",
        "            self.kernel = torch.tensor([[1, 2, 1],\n",
        "                                        [2, 4, 2],\n",
        "                                        [1, 2, 1]], device='cuda').unsqueeze(0).unsqueeze(0).half()\n",
        "            self.conv = F.conv2d\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(1, 1, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x, select_num=None, last=False):\n",
        "        B, patch_num = x.shape[0], x.shape[3] - 1\n",
        "        select_num = self.vote_perhead if select_num is None else select_num\n",
        "        count = torch.zeros((B, patch_num), dtype=torch.int, device='cuda').half()\n",
        "        score = x[:, :, 0, 1:]\n",
        "        _, select = torch.topk(score, self.vote_perhead, dim=-1)\n",
        "        select = select.reshape(B, -1)\n",
        "\n",
        "        for i, b in enumerate(select):\n",
        "            # b = b.cuda()\n",
        "            count[i, :] += torch.bincount(b, minlength=patch_num)\n",
        "\n",
        "        if not last:\n",
        "            count = self.enhance_local(count)\n",
        "\n",
        "        patch_value, patch_idx = torch.sort(count, dim=-1, descending=True)\n",
        "        patch_idx += 1\n",
        "        return patch_idx[:, :select_num], count\n",
        "\n",
        "    def enhance_local(self, count):\n",
        "        # B, H = count.shape[0], math.ceil(math.sqrt(count.shape[1]))\n",
        "        # count = count.reshape(B, H, H)\n",
        "        B = count.shape[0]\n",
        "        H = int(math.sqrt(count.shape[1]))\n",
        "        if self.fix:\n",
        "            count = self.conv(count.unsqueeze(1).unsqueeze(1), self.kernel, stride=1, padding=1).reshape(B, -1)\n",
        "        else:\n",
        "            count = self.conv(count.unsqueeze(1)).reshape(B, -1)\n",
        "        return count\n",
        "\n",
        "\n",
        "class CrossLayerRefinement(nn.Module):\n",
        "    def __init__(self, config, clr_layer):\n",
        "        super(CrossLayerRefinement, self).__init__()\n",
        "        self.clr_layer = clr_layer\n",
        "        self.clr_norm = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "    def forward(self, complements, cls_token):\n",
        "        out = [torch.stack(token) for token in complements]\n",
        "        out = torch.stack(out).squeeze(1)\n",
        "        out = torch.cat((cls_token, out), dim=1)\n",
        "        out, weights = self.clr_layer(out)\n",
        "        out = self.clr_norm(out)\n",
        "        return out, weights\n"
      ],
      "metadata": {
        "id": "BcFghfnyjmZr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 1\n",
        "timesteps = 100\n",
        "emb_size = 128\n",
        "num_classes = 2\n",
        "\n",
        "input_tensor = torch.rand(batch_size, timesteps, emb_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "input_tensor = input_tensor.to(device)\n",
        "\n",
        "class Config:\n",
        "    hidden_size = emb_size\n",
        "    num_layers = 6\n",
        "    num_heads = 8\n",
        "\n",
        "config = Config()\n",
        "\n",
        "\n",
        "model = AudioInterEnsembleLearningTransformer(config, num_classes=num_classes, emb_size=emb_size, timesteps=timesteps)\n",
        "model.to(device)\n",
        "\n",
        "output, attn_weights = model(input_tensor)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "5c6yfwzMj4lD",
        "outputId": "349ab7a2-6f23-4cad-95c5-707fe86c2284"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 100, 128])\n",
            "Positional embedding shape: torch.Size([1, 100, 128])\n",
            "Shape after positional embedding: torch.Size([1, 100, 128])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9d7e2a1ec7b0>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-968bafc1f64a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massess_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoder output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-968bafc1f64a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, test_mode)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_num\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Using ConformerBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mselect_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5c3ead35d7c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mattn_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5c3ead35d7c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5c3ead35d7c6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# now the shape is [B, h, 1, d//h]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mht_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mht_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhead_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    203\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2574\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m         )\n\u001b[0;32m-> 2576\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m def rms_norm(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    }
  ]
}